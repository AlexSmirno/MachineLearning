{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset snacks (C:/Users/79035/.cache/huggingface/datasets/Matthijs___snacks/default/0.0.1/c0ce49075aa469a098a5f2e3455941c894e02e1c9bf642d4d33e6c51460ff590)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbfd56d2f0944bab8aa1fe867f25afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install -q datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Matthijs/snacks') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple',\n",
       " 'banana',\n",
       " 'cake',\n",
       " 'candy',\n",
       " 'carrot',\n",
       " 'cookie',\n",
       " 'doughnut',\n",
       " 'grape',\n",
       " 'hot dog',\n",
       " 'ice cream',\n",
       " 'juice',\n",
       " 'muffin',\n",
       " 'orange',\n",
       " 'pineapple',\n",
       " 'popcorn',\n",
       " 'pretzel',\n",
       " 'salad',\n",
       " 'strawberry',\n",
       " 'waffle',\n",
       " 'watermelon']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 apple\n",
      "1 banana\n",
      "2 cake\n",
      "3 candy\n",
      "4 carrot\n",
      "5 cookie\n",
      "6 doughnut\n",
      "7 grape\n",
      "8 hot dog\n",
      "9 ice cream\n",
      "10 juice\n",
      "11 muffin\n",
      "12 orange\n",
      "13 pineapple\n",
      "14 popcorn\n",
      "15 pretzel\n",
      "16 salad\n",
      "17 strawberry\n",
      "18 waffle\n",
      "19 watermelon\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"test\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    print(i, label)\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: torch.Size([1, 3, 48, 48])\n",
      "Size after projection: torch.Size([1, 768, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "toy_img = torch.rand(1, 3, 48, 48)\n",
    "\n",
    "num_channels = 3\n",
    "hidden_size = 768\n",
    "patch_size = 16\n",
    "\n",
    "projection = nn.Conv2d(\n",
    "    num_channels, \n",
    "    hidden_size, \n",
    "    kernel_size=patch_size,\n",
    "    stride=patch_size)\n",
    "\n",
    "out_projection = projection(toy_img)\n",
    "\n",
    "print(f'Original image size: {toy_img.size()}')\n",
    "print(f'Size after projection: {out_projection.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embedding size: torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "# Flatten the output after projection with Conv2D layer\n",
    "\n",
    "patch_embeddings = out_projection.flatten(2).transpose(1, 2)\n",
    "print(f'Patch embedding size: {patch_embeddings.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embedding size: torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# Define [CLS] token embedding with the same emb dimension as the patches\n",
    "batch_size = 1\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "cls_tokens = cls_token.expand(batch_size, -1, -1)\n",
    "\n",
    "# Prepend [CLS] token in the beginning of patch embedding\n",
    "patch_embeddings = torch.cat((cls_tokens, patch_embeddings), dim=1)\n",
    "print(f'Patch embedding size: {patch_embeddings.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embedding size: torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# Define position embedding with the same dimension as the patch embedding\n",
    "position_embeddings = nn.Parameter(torch.randn(batch_size, 10, hidden_size))\n",
    "\n",
    "# Add position embedding into patch embedding\n",
    "input_embeddings = patch_embeddings + position_embeddings\n",
    "print(f'Input embedding size: {input_embeddings.size()}')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Output embedding size: torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for ViT-base (example)\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "\n",
    "# Define Transformer encoders' stack\n",
    "transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "           d_model=hidden_size, nhead=num_heads,\n",
    "           dim_feedforward=int(hidden_size * 4),\n",
    "           dropout=0.1)\n",
    "transformer_encoder = nn.TransformerEncoder(\n",
    "           encoder_layer=transformer_encoder_layer,\n",
    "           num_layers=num_layers)\n",
    "\n",
    "# Forward pass\n",
    "output_embeddings = transformer_encoder(input_embeddings)\n",
    "print(f' Output embedding size: {output_embeddings.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1186, -0.2206,  0.0520,  ...,  0.1596,  0.0328, -0.3451],\n",
      "         [ 0.1805, -0.0314,  0.0334,  ..., -0.0906,  0.1243, -0.2328],\n",
      "         [ 0.1073, -0.2869,  0.0526,  ...,  0.0268,  0.2267, -0.3698],\n",
      "         ...,\n",
      "         [ 0.1307, -0.1462, -0.0171,  ...,  0.0115,  0.0824, -0.3763],\n",
      "         [ 0.0577, -0.2698, -0.0516,  ..., -0.0277,  0.0176, -0.2410],\n",
      "         [ 0.1788, -0.1041,  0.0293,  ..., -0.0285,  0.0703, -0.1898]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, attentions=None)\n",
      "Ouput embedding size: torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from transformers import ViTModel\n",
    "\n",
    "# Load pretrained model\n",
    "model_checkpoint = 'google/vit-base-patch16-224-in21k'\n",
    "model = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)\n",
    "\n",
    "# Example input image\n",
    "input_img = torch.rand(batch_size, num_channels, 224, 224)\n",
    "\n",
    "# Forward pass input image\n",
    "output_embedding = model(input_img)\n",
    "print(output_embedding)\n",
    "print(f\"Ouput embedding size: {output_embedding['last_hidden_state'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output embedding size: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "num_labels = 20\n",
    "\n",
    "# Define linear classifier layer\n",
    "classifier = nn.Linear(hidden_size, num_labels) \n",
    "\n",
    "# Forward pass on the output embedding of [CLS] token\n",
    "output_classification = classifier(output_embedding['last_hidden_state'][:, 0, :])\n",
    "print(f\"Output embedding size: {output_classification.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Pretrained model checkpoint\n",
    "model_checkpoint = 'google/vit-base-patch16-224-in21k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, input_data):\n",
    "        \n",
    "      self.input_data = input_data\n",
    "      # Transform input data\n",
    "      self.transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                             std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.input_data)\n",
    "    \n",
    "  def get_images(self, idx):\n",
    "      return self.transform(self.input_data[idx]['image'])\n",
    "  \n",
    "  def get_labels(self, idx):\n",
    "      return self.input_data[idx]['label']\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "      # Get input data in a batch\n",
    "      train_images = self.get_images(idx)\n",
    "      train_labels = self.get_labels(idx)\n",
    "\n",
    "      return train_images, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "\n",
    "  def __init__(self, config=ViTConfig(), num_labels=20, \n",
    "               model_checkpoint='google/vit-base-patch16-224-in21k'):\n",
    "\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)\n",
    "        self.classifier = (\n",
    "            nn.Linear(config.hidden_size, num_labels) \n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = self.vit(x)['last_hidden_state']\n",
    "    # Use the embedding of [CLS] token\n",
    "    output = self.classifier(x[:, 0, :])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  1%|          | 4/605 [00:44<1:51:33, 11.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\МИСиС\\MachineLearning\\lab_4.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m trained_model \u001b[39m=\u001b[39m model_train(dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], EPOCHS, LEARNING_RATE, BATCH_SIZE)\n",
      "\u001b[1;32md:\\МИСиС\\MachineLearning\\lab_4.ipynb Cell 15\u001b[0m in \u001b[0;36mmodel_train\u001b[1;34m(dataset, epochs, learning_rate, bs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m total_acc_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m total_loss_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_train(dataset, epochs, learning_rate, bs):\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Load nodel, loss function, and optimizer\n",
    "    model = ViT().to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Load batch image\n",
    "    train_dataset = ImageDataset(dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    # Fine tuning loop\n",
    "    for i in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0.0\n",
    "\n",
    "        for train_image, train_label in tqdm(train_dataloader):\n",
    "            output = model(train_image.to(device))\n",
    "            loss = criterion(output, train_label.to(device))\n",
    "            acc = (output.argmax(dim=1) == train_label.to(device)).sum().item()\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f'Epochs: {i + 1} | Loss: {total_loss_train / len(train_dataset): .3f} | Accuracy: {total_acc_train / len(train_dataset): .3f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Train the model\n",
    "trained_model = model_train(dataset['train'], EPOCHS, LEARNING_RATE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                             std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    img = transform(img)\n",
    "    output = trained_model(img.unsqueeze(0).to(device))\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "\n",
    "    return id2label[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'][900]['image']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
