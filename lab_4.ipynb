{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dfc8bae",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4\n",
    "\n",
    "ФИО:  **Селифонов Артём Евгеньевич**\n",
    "Группа: **БИВТ-20-1**\n",
    "\n",
    "Отправлять можно следующими способами (**обязательно указать свое ФИО и группу в каком-либо виде**):\n",
    "1. Создать **приватный** репозиторий на github, добавить меня по нику (l3lush) в Collaborators (Settings -> Collaborators -> Add people)\n",
    "2. Отправить заполненный ноутбук мне на почту avmysh@gmail.com, либо m1603956@edu.misis.ru\n",
    "3. Отправить заполненный ноутбук мне в тг @l3lush. \n",
    "\n",
    "**Deadline**:\n",
    "- hard -- **04.06.2023 23:59** (дедлайн теперь один)\n",
    "\n",
    "\n",
    "**Что необходимо сделать** (можете вдохновляться ноутбуками для семинара, они должны помочь):\n",
    "1. Загрузить датасет (вариант смотреть [здесь](https://docs.google.com/spreadsheets/d/1pFk1qZJtMrV8GWUmdSjV5Kz6JnFdBQDShErFZ337FDc/edit?usp=sharing))\n",
    "2. Описать кратенько словами датасет, описать поставку задачи, что от чего отличаем, привести примеры картинок (картинки можно визуализировать после шага 3, когда у вас будет красивый датасет).\n",
    "3. Оформить датасет в виде объекта класса Dataset из PyTorch (обязательно надо сделать препроцессинг данных: нормализовать данные, добавить аугментации к данным и пр.).\n",
    "4. Оформить датасет из шага 3 в Dataloader.\n",
    "5. Реализовать архитектуру собственной нейросети и натренировать ее на датасете (можно не обучать 1000 эпох, достаточно 10 эпох, но чтобы метрики начали улучшаться).\n",
    "6. Обучить нейросеть, используя Transfer Learning. Модель можно выбрать на свой вкус (список всех моделей, доступных в torchvision есть [тут](https://pytorch.org/vision/stable/models.html)).\n",
    "7. Посчитать метрики качества финальной модели, сделать выводы.\n",
    "\n",
    "**Замечание**  \n",
    "Если понимаете, что данные слишком много весят, или с датасетом что-то не так, можете брать любой другой.  \n",
    "\n",
    "P.S. Чтобы не ждать века, тренируйте модели на Colab с использованием GPU (Runtime -> Change runtime type -> GPU)  \n",
    "P.S.S. Сохраняйте вывод ячеек и пушьте вместе с ним, в противном случае я не смогу нормально проверить все работы, если буду запускать все ноутбуки и ждать обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14441de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset snacks (C:/Users/79035/.cache/huggingface/datasets/Matthijs___snacks/default/0.0.1/c0ce49075aa469a098a5f2e3455941c894e02e1c9bf642d4d33e6c51460ff590)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ec0a66d263455e85ad6bce3aebd222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install -q datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Matthijs/snacks') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9225bcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 apple\n",
      "1 banana\n",
      "2 cake\n",
      "3 candy\n",
      "4 carrot\n",
      "5 cookie\n",
      "6 doughnut\n",
      "7 grape\n",
      "8 hot dog\n",
      "9 ice cream\n",
      "10 juice\n",
      "11 muffin\n",
      "12 orange\n",
      "13 pineapple\n",
      "14 popcorn\n",
      "15 pretzel\n",
      "16 salad\n",
      "17 strawberry\n",
      "18 waffle\n",
      "19 watermelon\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"test\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    print(i, label)\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T19:09:03.431720Z",
     "start_time": "2023-05-18T19:09:03.062838Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00cd0432",
   "metadata": {},
   "source": [
    "# Шаги 1-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### cifar100\n",
    "\n",
    "Этот датасет содержит 100 классов, содержащих по 600 изображений в каждом. На каждый класс приходится 500 train изображений и 100 test изображений. 100 классов в CIFAR-100 сгруппированы в 20 суперклассов. Каждое изображение снабжено меткой \"fine\" (класс, к которому оно принадлежит) и меткой \"coarse\" (суперкласс, к которому оно принадлежит)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9518c759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T21:10:07.550377Z",
     "start_time": "2023-05-18T21:10:05.876169Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\МИСиС\\MachineLearning\\lab_4.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_transforms \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     transforms\u001b[39m.\u001b[39mRandomHorizontalFlip(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize([\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], [\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m test_transforms \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize([\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], [\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m], batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transform \u001b[39m=\u001b[39;49m test_transforms)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m], batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, transform \u001b[39m=\u001b[39m train_transforms)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m class_names \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mfeatures[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mnames\n",
      "\u001b[1;31mTypeError\u001b[0m: DataLoader.__init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = 'C:\\\\Users\\\\79035\\\\.cache\\\\huggingface\\\\datasets\\\\Matthijs___snacks\\\\default\\\\0.0.1\\\\c0ce49075aa469a098a5f2e3455941c894e02e1c9bf642d4d33e6c51460ff590\\\\snacks-test.arrow'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "train_dataloader = DataLoader(dataset[\"test\"], batch_size=16, shuffle=True, transforms=test_transforms)\n",
    "test_dataloader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=False, transforms = train_transforms)\n",
    "\n",
    "class_names = dataset[\"test\"].features[\"label\"].names\n",
    "dataset_sizes = {\n",
    "    'train': len(dataset['test']),\n",
    "    'test': len(dataset['train'])\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "for data in train_dataloader:\n",
    "    print(str(data['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T20:02:26.703550Z",
     "start_time": "2023-05-18T20:02:26.601935Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:150\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[1;32m--> 150\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\МИСиС\\MachineLearning\\lab_4.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     plt\u001b[39m.\u001b[39mpause(\u001b[39m0.001\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#inputs, classes = next(iter(train_dataloader))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%D0%9C%D0%98%D0%A1%D0%B8%D0%A1/MachineLearning/lab_4.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataloader)))\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:130\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n\u001b[0;32m    131\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(elem, \u001b[39m'\u001b[39m\u001b[39m_fields\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# namedtuple\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m elem_type(\u001b[39m*\u001b[39m(collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)))\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:130\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n\u001b[0;32m    131\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(elem, \u001b[39m'\u001b[39m\u001b[39m_fields\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# namedtuple\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m elem_type(\u001b[39m*\u001b[39m(collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)))\n",
      "File \u001b[1;32mc:\\Users\\79035\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:150\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    148\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[1;32m--> 150\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>"
     ]
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "\n",
    "#inputs, classes = next(iter(train_dataloader))\n",
    "\n",
    "print(next(iter(train_dataloader)))\n",
    "#out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "#imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T20:03:39.450946Z",
     "start_time": "2023-05-18T20:03:39.447474Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            current_dataloader = train_dataloader\n",
    "            if phase == 'test':\n",
    "                current_dataloader = test_dataloader\n",
    "\n",
    "\n",
    "            for inputs, labels in current_dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 5),\n",
    "    nn.ELU(),\n",
    "    nn.Conv2d(16, 32, 5),\n",
    "    nn.ELU(),\n",
    "    nn.Conv2d(32, 128, 3, 3, 2),\n",
    "\n",
    "    nn.Conv2d(128, 128, 5),\n",
    "    nn.ELU(),\n",
    "    nn.Conv2d(128, 128, 5),\n",
    "    nn.ELU(),\n",
    "    nn.Conv2d(128, 128, 3, 3, 2),\n",
    "\n",
    "    nn.Conv2d(128, 128, 3),\n",
    "    nn.ELU(),\n",
    "    nn.Conv2d(128, 128, 3),\n",
    "    nn.ELU(),\n",
    "    nn.Conv2d(128, 128, 3, 3),\n",
    "\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(4608, 512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512, 2)\n",
    "\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1)\n",
    "model_fitted = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
